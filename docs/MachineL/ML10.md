# ML10 - 多元梯度下降 (Gradient descent for multiple variables)

Hypothesis: $ \quad h_{\theta}(x) = \theta^T x = \theta_0 + \theta_{1}x_1 + \theta_{2}x_2 + \cdots + \theta_{n}x_n $  
Parameters: $ \quad \theta_0, \ \theta_1, \cdots, \ \theta_n $  
Cost Function: $ J(\theta_0, \ \theta_1, \cdots, \ \theta_n)=\frac{1}{2m} \displaystyle\sum_{i=1}^m ( h_{\theta}(x^{(i)})-y^{(i)})^2 $  
Gradient descent:  
Repeat {
    $$
        \theta_j \ := \ \theta_j - \alpha \frac{\partial}{\partial \theta_j} J(\theta_0, \ \theta_1, \cdots, \ \theta_n) \quad (simultaneously update for every \ j = 0, \ \cdots, \ n)
    $$
}

### Gradient Descent:

Previously(n=1):  
Repeat {
    $$
    \theta_0 := \theta_0 - \alpha \frac{1}{m} \displaystyle\sum_{i=1}^m ( h_{\theta}(x^{(i)}) - y^{(i)}) \\ 
    \theta_1 := \theta_1 - \alpha \frac{1}{m} \displaystyle\sum_{i=1}^m ( h_{\theta}(x^{(i)}) - y^{(i)}) \cdot x^{(i)} \quad (simultaneously update \theta_0, \ \theta_1)
    $$
}

New algorithm (n $\le$ 1):  
Repeat {
    $$
        \theta_j \ := \ \theta_j - \alpha \frac{1}{m} \displaystyle\sum_{i=1}^m ( h_{\theta}(x^{(i)}) - y^{(i)}) \cdot x_j^{(i)} \\(simultaneously \ update \ \theta_j \ for \ j = 0, \ \cdots, \ n)
    $$
}