# ML37 - Neural Networks: Learning - Backpropagation algorithm

### 反向传播算法

**Gradient computation**: 
$$
\mathtt{J}(\theta) = - \frac{1}{m} \begin{bmatrix} \displaystyle\sum_{i=1}^m\sum_{k=1}^K y_k^{(i)}log(h_\theta(x^{(i)}))_k + (1-y_k^{(i)})log(1-(h_\theta(x^{(i)}))_k) \end{bmatrix} + \frac{\lambda}{2m}\displaystyle\sum_{l=1}^{L-1}\sum_{i=1}^{s_l}\sum_{j=1}^{s_l+1} (\theta_{ji}^{(l)})^2
$$

$ \displaystyle\min_\theta \mathsf{J}(\theta) $

**Need code to compute**:  
- $\mathtt{J}(\theta)$
- $\frac{\partial}{\partial\theta_{ij}^{(l)}} \mathtt{J}(\theta)$

**Gradient computation: Backpropagation algorithm**:  
Intuition: $\delta_j^{(l)}$ = "error" of node j in layer l.