# ML25 - Logistic Regression Advanced optimization

### 高级优化

##### Optimization algorithm
Cost function $J(\theta)$. Want $\displaystyle\min_\theta J(\theta)$.  
Given $\theta$, we have code that can compute  
    $ \quad \quad - \ J(\theta)$  
    $ \quad \quad - \ \frac{\partial}{\partial\theta_j} J(\theta) \quad (for \ j \ = \ 1, \ 2, \ \cdots, \ n)$

**Optimization algorithms**:
- Gradient descent
- Conjugate gradient (共轭梯度法)
- BFGS 
- L-BFGS 

Advantages:
- No need to manually pick $\alpha$.
- Often faster than gradient descent.

Disadvantages:
- More complex

##### Gradient Descent
Repeat {  
    $\quad \quad \quad \theta_j \ := \ \theta_j - \alpha\frac{\partial}{\partial\theta_j}J(\theta) $  
}