# ML48 - Advice for applying machine learning - Regularization and bias/variance

### 正则化和偏差、方差

**Linear regression with regularization**  
Model: $ h_\theta(x) = \theta_0 + \theta_1x + \theta_2x^2 + \theta_3x^3 + \theta_4x^4 $  
$ \quad \quad \ \ J(\theta) = \frac{1}{2m} \displaystyle\sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})^2 + \frac{\lambda}{2m} \displaystyle\sum_{j=1}^m \theta_j^2 $  
$ \quad \quad \ \ J_{train} (\theta) = \frac{1}{2m} \displaystyle\sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})^2 $  
$ \quad \quad \ \ J_{cv} (\theta) = \frac{1}{2m_{cv}} \displaystyle\sum_{i=1}^{m_{cv}} (h_\theta(x_{cv}^{(i)}) - y_{cv}^{(i)})^2 $  
$ \quad \quad \ \ J_{test} (\theta) = \frac{1}{2m_{test}} \displaystyle\sum_{i=1}^{m_{test}} (h_\theta(x_{test}^{(i)}) - y_{test}^{(i)})^2 $  

**Bias/variance as a function of the regularization parameter** $ \lambda $  
$ J(\theta) = \frac{1}{2m} \displaystyle\sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})^2 + \frac{\lambda}{2m} \displaystyle\sum_{j=1}^m \theta_j^2 $  
$ J_{train} (\theta) = \frac{1}{2m} \displaystyle\sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})^2 $  
$ J_{cv} (\theta) = \frac{1}{2m_{cv}} \displaystyle\sum_{i=1}^{m_{cv}} (h_\theta(x_{cv}^{(i)}) - y_{cv}^{(i)})^2 $