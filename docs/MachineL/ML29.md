# ML29 - Regularization - Regularized linear regression

### 线性回归的正则化

**Regularized linear regression**  
$$
 J(\theta) = \frac{1}{2m} \begin{bmatrix} \displaystyle\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2 + \lambda\displaystyle\sum_{i=1}^n\theta_j^2 \end{bmatrix} 
$$

$$
 \displaystyle\min_\theta J(\theta)
$$

**Gradient descent**  

Repeat {
    $$
    \theta_0 := \theta_0 - \alpha \frac{1}{m} \displaystyle\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_0^{(i)} \\
    \theta_j := \theta_j - \alpha \begin{bmatrix}  \frac{1}{m} \displaystyle\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)} + \frac{\lambda}{m}\theta_j \end{bmatrix} \quad (j = . , \ 1, \ 2, \ \cdots, \ n)
    $$
}  
$$ \theta_j := \theta_j(1-\alpha\frac{\lambda}{m}) - \alpha\frac{1}{m} \displaystyle\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)} $$

**Non-invertibility (optional/advanced)**  
Suppose $ m \le n \ , \ \theta = (X^T X)^{-1}X^Ty $
if $ \lambda > 0 $,
$$
\theta = \begin{pmatrix} X^T X + \lambda 
\begin{bmatrix}
{0}&{}&{}&{}&{}\\
{}&{1}&{}&{}&{}\\
{}&{}&{\ddots}&{}&{}\\
{}&{}&{}&{1}&{}\\
{}&{}&{}&{}&{1}\\
\end{bmatrix}
\end{pmatrix}^{-1}X^T y
$$