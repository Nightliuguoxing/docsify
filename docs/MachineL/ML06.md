# ML06 - 梯度下降 (Gradient Descent)

Have some function $ J(\theta_0, \ \theta_1, \cdots, \theta_n) \quad >>> \quad J(\theta_0, \ \theta_1) $  
Want $$ \min_{\theta_0, \ \theta_1, \cdots, \theta_n} J(\theta_0, \ \theta_1, \cdots, \theta_n) \quad >>> \quad \min_{\theta_0, \ \theta_1} J(\theta_0, \ \theta_1) $$

**Outline:**
- Start with some $ \theta_0, \ \theta_1$
- Keep changing $ \theta_0, \ \theta_1 $ to reduce $ J(\theta_0, \ \theta_1) $ until we hopefully end up at a minimum

### Gradient Descent Algorithm
repeat until convergence {
    $$
        \theta_j \ := \ \theta_j - \alpha \frac{\partial}{\partial \theta_j} J(\theta_0, \ \theta_1) \quad (for \ j = 0 \ and \ j = 1)
    $$
}
---
Correct: Simultaneous update

temp0 := $ \theta_0 - \alpha \frac{\partial}{\partial \theta_0} J(\theta_0, \ \theta_1) $  
temp1 := $ \theta_1 - \alpha \frac{\partial}{\partial \theta_1} J(\theta_0, \ \theta_1) $  
$ \theta_0 $ := temp0  
$ \theta_1 $ := temp1 

### Incorrect:
> [!DANGER]
temp0 := $ \theta_0 - \alpha \frac{\partial}{\partial \theta_0} J(\theta_0, \ \theta_1) $  
$ \theta_0 $ := temp0  
temp1 := $ \theta_1 - \alpha \frac{\partial}{\partial \theta_1} J(\theta_0, \ \theta_1) $  
$ \theta_1 $ := temp1 

Gradient descent can converage to a local minimum, even with the learning rate $ \alpha $ fixed.  
$$ \theta_1 := \theta_1 - \alpha \frac{d}{d \theta_1} J(\theta_1) $$
As we approach a local minimum, gradient descent will automatically take smaller steps.  
So, no need to decrease $ \alpha $ over time.