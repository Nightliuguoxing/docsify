# ML07 - 线性回归 + 梯度下降

### Gradient Descent Algorithm
repeat until convergence {
    $$
        \theta_j \ := \ \theta_j - \alpha \frac{\partial}{\partial \theta_j} J(\theta_0, \ \theta_1) \quad (for \ j = 0 \ and \ j = 1)
    $$
}

### Linear Regression Model
$$ \quad h_{\theta}(x) = \theta_0 + \theta_{1}x $$
$$ J(\theta_0, \ \theta_1) = \frac{1}{2m} \displaystyle\sum_{i=1}^m ( h_{\theta}(x^{(i)}) - y^{(i)})^2 $$
$$ \min_{\theta_0 \theta_1} J(\theta_0, \ \theta_1) $$

---

$$
\frac{\partial}{\partial \theta_j} J(\theta_0, \ \theta_1) 
= \frac{\partial}{\partial \theta_j} \cdot \frac{1}{2m} \displaystyle\sum_{i=1}^m ( h_{\theta}(x^{(i)}) - y^{(i)})^2 \\
\quad \quad \quad \quad \quad \quad \quad = \frac{\partial}{\partial \theta_j} \cdot \frac{1}{2m} \displaystyle\sum_{i=1}^m ( \theta_0 + \theta_{1}x^{(i)} - y^{(i)})^2 \\
$$

$$ \theta_0 \quad j \ = \ 0 \ : \ \frac{\partial}{\partial \theta_0} J(\theta_0, \ \theta_1) = \frac{1}{m} \displaystyle\sum_{i=1}^m ( h_{\theta}(x^{(i)}) - y^{(i)}) $$
$$ \theta_1 \quad j \ = \ 1 \ : \ \frac{\partial}{\partial \theta_1} J(\theta_0, \ \theta_1) = \frac{1}{m} \displaystyle\sum_{i=1}^m ( h_{\theta}(x^{(i)}) - y^{(i)}) \cdot x^{(i)} $$

---

repeat until convergence {
    $$ update \ \theta_0 \ and \ \theta_1 \ simultaneously
    \begin{cases}
    \theta_0 := \theta_0 - \alpha \frac{1}{m} \displaystyle\sum_{i=1}^m ( h_{\theta}(x^{(i)}) - y^{(i)}) \\ 
    \theta_1 := \theta_1 - \alpha \frac{1}{m} \displaystyle\sum_{i=1}^m ( h_{\theta}(x^{(i)}) - y^{(i)}) \cdot x^{(i)} 
    \end{cases}
    $$
}