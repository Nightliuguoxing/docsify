# ML24 - Logistic Regression Simplified cost function and gradient decent

### 简化代价函数与梯度下降

### Logistic Regression cost function
$ J(\theta) = \frac{1}{m} \displaystyle\sum_{i=1}^m Cost(h_{\theta}(x^{(i)}), \ y^{(i)}) $  
$ Cost(h_\theta(x), \ y) = 
\begin{cases}
\quad \ \ -log(h_\theta(x)) \quad if \ y \ = \ 1 \\
-log(1 - h_\theta(x)) \quad if \ y \ = \ 0
\end{cases}
$  
Note: y = 0 or 1 always

---
$ Cost(h_\theta(x), \ y) = 
\begin{cases}
\quad \ \ -log(h_\theta(x)) \quad if \ y \ = \ 1 \\
-log(1 - h_\theta(x)) \quad if \ y \ = \ 0
\end{cases} \\
\quad \quad \quad \quad \quad \quad \ \ = -ylog(h_\theta(x)) - (1-y)log(1-h_\theta(x))
$

$ J(\theta) = \frac{1}{m} \displaystyle\sum_{i=1}^m Cost(h_{\theta}(x^{(i)}), \ y^{(i)}) $  
$ \quad \quad \ = -\frac{1}{m}[\displaystyle\sum_{i=1}^m y^{(i)}logh_\theta(x^{(i)}) + (1-y^{(i)})log(1-h_\theta(x^{(i)}))]  $  

##### To fit parameters $\theta$:
$\quad \quad \quad \displaystyle\min_\theta J(\theta) $

##### To make a prediction given new $x$:
Output $ \quad h_\theta(x) = \frac{1}{1+e^{-\theta^Tx}} $

##### Gradient Descent
$ J(\theta) = -\frac{1}{m}[\displaystyle\sum_{i=1}^m y^{(i)}logh_\theta(x^{(i)}) + (1-y^{(i)})log(1-h_\theta(x^{(i)}))]  $  

Want $\displaystyle\min_\theta J(\theta)$ :
Repeat {  
    $\quad \quad \quad \theta_j \ := \ \theta_j - \alpha\frac{\partial}{\partial\theta_j}J(\theta) $  
    $\quad \quad \quad \theta_j \ := \ \theta_j - \alpha\displaystyle\sum_{i=1}^m(h_{\theta}(x^{(i)}) - y^{(i)})x_j^{(i)} $  
    $\quad \quad $(simultaneously update all $\theta_j$)  
}