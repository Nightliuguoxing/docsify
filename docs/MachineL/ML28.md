# ML28 - Regularization - Cost function

### 代价函数

**Regularization**.  
Small value for parameters $ \theta_0, \ \theta_1, \ \cdots, \ \theta_n $
- "Simpler" hypothesis
- Less prone to overfitting

Housing:
- Features: $x_1, \ x_2, \ \cdots, \ x_100$
- Parameters: $\theta_0, \ \theta_1, \ \theta_2, \ \cdots, \ \theta_100$
- $ J(\theta) = \frac{1}{2m} \begin{bmatrix} \displaystyle\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2 + \lambda\displaystyle\sum_{i=1}^n\theta_j^2 \end{bmatrix}$

In regularized linear regression, we choose $ \theta $ to minimize
$$
 J(\theta) = \frac{1}{2m} \begin{bmatrix} \displaystyle\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2 + \lambda\displaystyle\sum_{i=1}^n\theta_j^2 \end{bmatrix}
$$
What if $ \lambda $ is set to an extremely large value (perhaps for too large for our problem, say $ \lambda = 10^{10} $)?
