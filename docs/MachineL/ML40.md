# ML40 - Neural Networks: Learning - Gradient checking

### 梯度检测

**Numerical estimation of gradients**  

Implement: 

```octave
gradApprox = ( J (theta + EPSILON) - J (theta - EPSILON)) / (2 * EPSILON)
```

**Parameter vector $\vartheta $**  
$ \quad \quad \theta \in \mathbb{R}^n $ E.G. $ \theta $ is "unroll" version of $ \Theta^{(1)}, \ \Theta^{(2)}, \ \Theta^{(3)} $

```octave
for i = 1 : n,
    thetaPlus = theta;
    thetaPlus(i) = thetaPlus(i) + EPSILON;
    thetaMinus = theta;
    thetaMinus(i) = thetaMinus(i) - EPSILON;
    gradApprox(i) = (J(thetaPlus) - J(thetaMinus)) / (2 * EPSILON);
end;
```

Check that `gradApprox` $\approx$ `DVec`

Implementation Note:	
- Implement backprop to compute DVec (unrolled $ \mathtt{D}^{(1)}, \ \mathtt{D}^{(2)}, \ \mathtt{D}^{(3)}  $).
-­ Implement numerical gradient check to compute gradApprox.	  
-­ Make ure they give similar values.	
-­ Turn off gradient checking. Using backprop code for learning.	
  
Important:  
- Be sure to disable your gradient checking code before training your classifier. If you run numerical gradient computation on every iteration of gradient descent (or in the inner loop of costFunction(…))your code will be very slow.	
  
